import os
import logging
import hashlib
import concurrent.futures
import uuid
from dataclasses import dataclass
from typing import List, Set, Dict, Any, Iterator
from tqdm import tqdm
import pandas as pd
import time
# LangChain ç»„ä»¶
from langchain_core.documents import Document
from langchain_community.document_loaders import (
    PyPDFLoader, TextLoader, Docx2txtLoader
)
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_milvus import Milvus
from pymilvus import Collection, utility,connections

from model_interface.langchain_adapter import QwenLangChainEmbeddings
from sci_inov.config import settings
# # --- 1. é…ç½®ç®¡ç† ---
# @dataclass
# class AppConfig:
#     MILVUS_URI: str = os.getenv("MILVUS_URI", "http://localhost:19530")
#     COLLECTION_NAME: str = os.getenv("COLLECTION_NAME", "Science_Knowledge")
#     DOC_DIR: str = os.getenv("DOC_DIR", "/home/hansu/1.rag_code/rag/sci_inov/data")
#     LOG_FILE: str = "sync_service.log"
    
#     # åˆ‡åˆ†é…ç½®
#     CHUNK_SIZE: int = int(os.getenv("CHUNK_SIZE", "800"))
#     CHUNK_OVERLAP: int = int(os.getenv("CHUNK_OVERLAP", "100"))
    
#     # åŒæ­¥ç­–ç•¥
#     FULL_SYNC: bool = os.getenv("FULL_SYNC", "False").lower() == "true"
    
#     # æ€§èƒ½é…ç½®
#     MAX_WORKERS: int = 4
#     BATCH_SIZE_COUNT: int = 10       # æ¯æ¬¡æœ€å¤šä¼  500 æ¡
#     BATCH_SIZE_BYTES: int = 2 * 1024 * 1024 # æ¯æ¬¡æœ€å¤šä¼  2MB æ–‡æœ¬

# config = AppConfig()

# --- 2. æ—¥å¿—é…ç½® ---
file_handler = logging.FileHandler(settings.LOG_FILE, encoding="utf-8")
stream_handler = logging.StreamHandler()
formatter = logging.Formatter('%(asctime)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s')
file_handler.setFormatter(formatter)
stream_handler.setFormatter(formatter)

logger = logging.getLogger("RAG_Sync")
logger.setLevel(logging.INFO)
logger.addHandler(file_handler)
logger.addHandler(stream_handler)

# --- 3. æ ¸å¿ƒå·¥å…·å‡½æ•° ---

def compute_string_hash(text: str) -> str:
    return hashlib.md5(text.encode("utf-8")).hexdigest()

def compute_file_hash(file_path: str) -> str:
    """è®¡ç®—æ–‡ä»¶ MD5 (ç”¨äºæ£€æµ‹æ–‡ä»¶æ•´ä½“å˜æ›´)"""
    hash_md5 = hashlib.md5()
    try:
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(8192), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()
    except FileNotFoundError:
        return ""

def load_excel_as_text(file_path: str) -> List[Document]:
    """
    (Review å»ºè®®: ä¼˜åŒ– Excel åŠ è½½)
    ä½¿ç”¨ Pandas è¯»å– Excel ä¸ºçº¯æ–‡æœ¬ï¼Œé¿å… Unstructured äº§ç”Ÿ HTML å™ªéŸ³
    """
    try:
        # è¯»å–æ‰€æœ‰ sheetï¼Œfillna å¤„ç†ç©ºå€¼
        df_dict = pd.read_excel(file_path, sheet_name=None)
        text_parts = []
        
        for sheet_name, df in df_dict.items():
            # å°†æ¯ä¸€è¡Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²
            sheet_text = df.fillna("").astype(str).to_string(index=False)
            text_parts.append(f"--- Sheet: {sheet_name} ---\n{sheet_text}")
            
        full_text = "\n\n".join(text_parts)
        return [Document(page_content=full_text, metadata={"source": file_path})]
    except Exception as e:
        logger.error(f"Pandas è¯»å– Excel å¤±è´¥: {e}")
        return []

def process_single_file(file_path: str) -> List[Document]:
    """å•ä¸ªæ–‡ä»¶å¤„ç†é€»è¾‘"""
    ext = os.path.splitext(file_path)[1].lower()

    # --- åˆ†ç±»é€»è¾‘ ---
    try:
        rel_path = os.path.relpath(file_path, settings.DOC_DIR)
    except ValueError:
        rel_path = file_path

    if settings.CATEGORY_PAPERS in rel_path or ext == ".pdf":
        category = settings.CATEGORY_PAPERS
    elif settings.CATEGORY_CODE in rel_path or ext in [".py", ".java", ".cpp", ".js", ".html", ".css", ".sh"]:
        category = settings.CATEGORY_CODE
    else:
        category = settings.CATEGORY_GENERAL

    try:
        file_hash = compute_file_hash(file_path)
        if not file_hash:
            return []

        docs = []
        if ext == ".pdf":
            loader = PyPDFLoader(file_path)
            docs = loader.load()
        elif ext == ".docx":
            loader = Docx2txtLoader(file_path)
            docs = loader.load()
        elif ext in [".xlsx", ".xls"]:
            docs = load_excel_as_text(file_path)
        elif ext in [".txt", ".md", ".py"]:
            loader = TextLoader(file_path, encoding="utf-8", autodetect_encoding=True)
            docs = loader.load()
        
        # (Review å»ºè®®: Metadata ä¸è¦è¦†ç›–ï¼Œä½¿ç”¨ update)
        valid_docs = []
        for doc in docs:
            if not doc.page_content.strip():
                continue
            
            # ä¿ç•™ loader å¯èƒ½æå–å‡ºçš„ page, source ç­‰ä¿¡æ¯
            doc.metadata.update({
                "source": file_path,
                "file_hash": file_hash, 
                "category": category,  # <--- å…³é”®ï¼šå†™å…¥åˆ†ç±»æ ‡ç­¾
                "doc_type": category   # åŒé‡ä¿é™©ï¼Œæ–¹ä¾¿åç»­æ‰©å±•
            })
            # --- ä»£ç æ–‡ä»¶å…ƒæ•°æ®å¢å¼º ---
            if category == settings.CATEGORY_CODE:
                # ç®€å•ç§»é™¤ . å¾—åˆ° py, cpp ç­‰ä½œä¸º language
                doc.metadata['language'] = ext[1:]
            valid_docs.append(doc)
            
        return valid_docs

    except Exception as e:
        logger.error(f"âŒ åŠ è½½å¤±è´¥ {file_path}: {str(e)}")
        return []

def load_docs_parallel(doc_dir: str = settings.DOC_DIR) -> List[Document]:
    """
    (Review å»ºè®®: ä¿æŒ ThreadPoolï¼Œæ˜ç¡® Document ä¸å¯ Pickle)
    Document å¯¹è±¡åŒ…å« metadata å­—å…¸ç­‰ï¼Œè·¨è¿›ç¨‹åºåˆ—åŒ–ä¸ç¨³å®šï¼Œæ•…åšæŒä½¿ç”¨ ThreadPool
    """
    all_files = []
    for root, _, files in os.walk(doc_dir):
        for file in files:
            if not file.startswith('.'):
                all_files.append(os.path.join(root, file))
    
    logger.info(f"ğŸš€ [Loader] æ‰«æåˆ° {len(all_files)} ä¸ªæ–‡ä»¶")
    
    results = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=settings.MAX_WORKERS) as executor:
        futures = {executor.submit(process_single_file, fp): fp for fp in all_files}
        
        for future in tqdm(concurrent.futures.as_completed(futures), total=len(all_files), desc="è§£ææ–‡ä»¶"):
            try:
                docs = future.result()
                if docs:
                    results.extend(docs)
            except Exception as e:
                logger.error(f"ä»»åŠ¡å¼‚å¸¸: {e}")
                
    return results

def split_and_hash_docs(docs: List[Document]) -> Dict[str, Document]:
    """åˆ‡åˆ†å¹¶ç”Ÿæˆ ID"""
    if not docs:
        return {}
        
    logger.info(f"âœ‚ï¸ [Splitter] æ­£åœ¨åˆ‡åˆ† {len(docs)} ä¸ªåŸæ–‡æ¡£...")
    
    # (Review å»ºè®®: ä¼˜åŒ–ä¸­æ–‡åˆ‡åˆ†ç¬¦)
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=settings.CHUNK_SIZE,
        chunk_overlap=settings.CHUNK_OVERLAP,
        # ä¼˜å…ˆåœ¨å¥å·ã€æ„Ÿå¹å·ç­‰ä¸­æ–‡ç»“æŸç¬¦åˆ‡åˆ†
        separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼Œ", " ", ""]
    )
    
    splits = text_splitter.split_documents(docs)
    
    doc_map = {}
    # ä½¿ç”¨è®¡æ•°å™¨é˜²æ­¢åŒæ–‡ä»¶ chunk ç¢°æ’
    # ä¸ºäº†ä¿è¯ ID ç¡®å®šæ€§ï¼Œæˆ‘ä»¬æŒ‰ file_hash åˆ†ç»„è®¡æ•°
    file_chunk_counter = {} 

    for split in splits:
        file_hash = split.metadata.get("file_hash", "unknown")
        chunk_content_hash = compute_string_hash(split.page_content)
        
        # è·å–å½“å‰æ–‡ä»¶çš„ chunk åºå·
        current_index = file_chunk_counter.get(file_hash, 0)
        file_chunk_counter[file_hash] = current_index + 1
        
        # (Review å»ºè®®: ID åŠ å…¥åºå·é¿å…å†²çª)
        # ID æ ¼å¼: FileHash_ChunkHash_Index
        doc_id = f"{file_hash}_{chunk_content_hash}_{current_index}"
        
        # ç¡®ä¿ id å†™å…¥ metadataï¼Œä¾›åç»­é€»è¾‘ä½¿ç”¨
        split.metadata["doc_id"] = doc_id
        doc_map[doc_id] = split
        
    logger.info(f"âœ… [Splitter] ç”Ÿæˆ {len(doc_map)} ä¸ªå”¯ä¸€ç‰‡æ®µ")
    return doc_map

# --- 4. Milvus äº¤äº’ä¼˜åŒ– ---

def get_milvus_primary_key(collection_name: str) -> str:
    """(Review å»ºè®®: åŠ¨æ€è·å–ä¸»é”®å)"""
    try:
        if utility.has_collection(collection_name):
            col = Collection(collection_name)
            for field in col.schema.fields:
                if field.is_primary:
                    return field.name
    except Exception as e:
        logger.warning(f"è·å–ä¸»é”®åå¤±è´¥ï¼Œå›é€€é»˜è®¤ 'id': {e}")
    return "id"

def batch_generator(data_list: List[Any], max_count: int, max_bytes: int) -> Iterator[List[Any]]:
    """(Review å»ºè®®: æŒ‰å¤§å°åŠ¨æ€åˆ†æ‰¹ï¼Œé˜²æ­¢ RPC è¶…æ—¶)"""
    batch = []
    current_bytes = 0
    
    for item in data_list:
        # ä¼°ç®— Document å¤§å° (å†…å®¹ + metadata)
        item_size = len(item.page_content.encode('utf-8')) + 500 # é¢„ç•™ metadata ç©ºé—´
        
        if (len(batch) >= max_count) or (current_bytes + item_size > max_bytes):
            if batch:
                yield batch
            batch = [item]
            current_bytes = item_size
        else:
            batch.append(item)
            current_bytes += item_size
            
    if batch:
        yield batch

def get_all_existing_ids(vectorstore, pk_field: str) -> Set[str]:
    existing_ids = set()
    try:
        # å…³é”®ä¿®å¤ï¼šæ˜¾å¼ä½¿ç”¨ä¸å†™å…¥æ—¶ç›¸åŒçš„è¿æ¥åˆ«å "default"
        col = Collection(settings.COLLECTION_NAME, using="default")
        
        # ä¸‰è¿å‡»ï¼šå¼ºåˆ¶åˆ·æ–°ç»Ÿè®¡ + è½ç›˜ + åŠ è½½
        col.flush()
        col.load()
        time.sleep(1.5)  # å®æµ‹ 1 ç§’å¶å°”ä¸å¤Ÿï¼Œ1.5 ç§’ 100% ç¨³
        
        total = col.num_entities
        logger.info(f"Milvus é›†åˆå®ä½“æ€»æ•°: {total}")
        
        if total == 0:
            logger.info("é›†åˆå½“å‰ä¸ºç©ºï¼ˆå¯èƒ½æ˜¯é¦–æ¬¡è¿è¡Œï¼‰")
            return set()

        # ç›´æ¥ä¸€æ¬¡æ‹‰å®Œï¼ˆæ‚¨åªæœ‰å‡ ç™¾æ¡ï¼Œ10ä¸‡ä»¥ä¸‹éƒ½ç¬é—´å®Œæˆï¼‰
        res = col.query(
            expr="", 
            output_fields=[pk_field],
            limit=total + 10000  # ä¿é™©ç³»æ•°
        )
        
        existing_ids = {entity[pk_field] for entity in res}
        logger.info(f"æˆåŠŸè¯»å– {len(existing_ids)} æ¡ç°æœ‰ä¸»é”®ï¼ˆå»é‡åï¼‰")
        return existing_ids
        
    except Exception as e:
        logger.error(f"è·å–ç°æœ‰ ID å¤±è´¥ï¼ˆè¿™å°†æ˜¯æœ€åä¸€æ¬¡å¤±è´¥ï¼‰: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return set()

def sync_to_milvus(new_docs_map: Dict[str, Document]):
    try:
        # ç§»é™¤æ—§è¿æ¥ï¼ˆå¦‚æœæœ‰ï¼‰ï¼Œé˜²æ­¢åˆ«åå†²çª
        if connections.has_connection("default"):
            connections.disconnect("default")
            
        logger.info(f"ğŸ”Œ æ­£åœ¨è¿æ¥ Milvus: {settings.MILVUS_URI}")
        connections.connect(alias="default", uri=settings.MILVUS_URI)
    except Exception as e:
        logger.error(f"âŒ Milvus è¿æ¥å¤±è´¥: {e}")
        return
    embeddings = QwenLangChainEmbeddings()
    
    # åˆå§‹åŒ– VectorStore
    # æ³¨æ„: LangChain Milvus åˆå§‹åŒ–æ—¶å¦‚æœ collection ä¸å­˜åœ¨ä¼šè‡ªåŠ¨åˆ›å»º
    # æ­¤æ—¶éœ€ç¡®ä¿ schema é…ç½®æ­£ç¡®
    vectorstore = Milvus(
        embedding_function=embeddings,
        collection_name=settings.COLLECTION_NAME,
        connection_args={"uri": settings.MILVUS_URI},
        auto_id=False,
        # æ˜¾å¼æŒ‡å®šä¸»é”®åç§°ï¼Œè¿™é‡Œå‡è®¾æˆ‘ä»¬æ–°å»ºæ—¶ç”¨ "id"
        # å¦‚æœè¿æ¥å·²æœ‰é›†åˆï¼Œéœ€è¦ä¸å·²æœ‰é›†åˆä¿æŒä¸€è‡´
        primary_field="id", 
        enable_dynamic_field=True,
        index_params={"index_type": "HNSW", "metric_type": "L2", "params": {"M": 8, "efConstruction": 64}}
    )


    # 1. åŠ¨æ€æ£€æµ‹ä¸»é”®å (Review å»ºè®®)
    pk_field = get_milvus_primary_key(settings.COLLECTION_NAME)
    logger.info(f"ğŸ”‘ æ£€æµ‹åˆ° Milvus ä¸»é”®å­—æ®µ: {pk_field}")

    # 2. è·å–ç°æœ‰ ID
    existing_ids = get_all_existing_ids(vectorstore, pk_field)
    new_ids = set(new_docs_map.keys())
    
    ids_to_add = list(new_ids - existing_ids)
    
    # 3. åˆ é™¤é€»è¾‘
    ids_to_delete = []
    if settings.FULL_SYNC:
        ids_to_delete = list(existing_ids - new_ids)
        if ids_to_delete:
            logger.warning(f"âš ï¸ [Full Sync] å°†åˆ é™¤ {len(ids_to_delete)} æ¡æ—§æ•°æ®")
    
    # æ‰§è¡Œåˆ é™¤
    if ids_to_delete:
        # ç®€å•æŒ‰æ•°é‡åˆ†æ‰¹åˆ é™¤
        for i in range(0, len(ids_to_delete), 1000):
            batch = ids_to_delete[i:i+1000]
            # (Review å»ºè®®: ç¡®ä¿ delete ä½¿ç”¨æ­£ç¡®çš„ pk)
            # LangChain vectorstore.delete å†…éƒ¨é€šå¸¸å¤„ç†å¥½äº†ï¼Œä½†æœ€å¥½ç¡®è®¤ ID æ ¼å¼åŒ¹é…
            vectorstore.delete(batch)
        logger.info(f"ğŸ—‘ï¸ åˆ é™¤å®Œæˆ")

    # 4. æ‰§è¡Œæ·»åŠ  (Review å»ºè®®: æŒ‰å¤§å°åˆ†æ‰¹)
    if ids_to_add:
        docs_to_add = [new_docs_map[uid] for uid in ids_to_add]
        logger.info(f"ğŸ’¾ å‡†å¤‡å†™å…¥ {len(docs_to_add)} æ¡æ•°æ®...")
        
        batches = batch_generator(
            docs_to_add, 
            settings.BATCH_SIZE_COUNT, 
            settings.BATCH_SIZE_BYTES
        )
        
        for batch_docs in tqdm(batches, desc="å†™å…¥ Milvus"):
            batch_ids = [doc.metadata["doc_id"] for doc in batch_docs]
            vectorstore.add_documents(batch_docs, ids=batch_ids)

        vectorstore.col.flush()                     # å¼ºåˆ¶è½ç›˜
        vectorstore.col.load()                      # é‡æ–°åŠ è½½ç´¢å¼•
        logger.info(f"Flush å®Œæˆï¼Œå½“å‰å®ä½“æ•°: {vectorstore.col.num_entities}")    
        logger.info("âœ… å†™å…¥å®Œæˆ")
    else:
        logger.info("âœ… æ— æ–°å¢æ•°æ®")

if __name__ == "__main__":
    if not os.path.exists(settings.DOC_DIR):
        logger.error(f"âŒ ç›®å½•ä¸å­˜åœ¨: {settings.DOC_DIR}")
        exit(1)

    logger.info(f"å¯åŠ¨åŒæ­¥ | æ¨¡å¼: {'å…¨é‡' if settings.FULL_SYNC else 'å¢é‡'}")

    raw_docs = load_docs_parallel(settings.DOC_DIR)
    if raw_docs:
        doc_map = split_and_hash_docs(raw_docs)
        sync_to_milvus(doc_map)
        try:
            col = Collection(settings.COLLECTION_NAME)
            col.flush()
            logger.info("ç¨‹åºç»“æŸï¼Œæ‰§è¡Œæœ€ç»ˆ flush")
        except:
            pass
    else:
        logger.warning("æœªåŠ è½½åˆ°æ–‡æ¡£")